{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmdiL2RaaYzB"
   },
   "source": [
    "**hf login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1765095848392,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "v3TDVaPkbmOf"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token= \"paste your hf token here\"\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpLODJ7zaYv5"
   },
   "source": [
    "**install required package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 242322,
     "status": "ok",
     "timestamp": 1765096141610,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "72wfh2LDbm0o",
    "outputId": "7ad6e3d4-c86f-48b3-b701-164e68cac70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m817.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes torch transformers accelerate einops wandb\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKSSt8LlaYsR"
   },
   "source": [
    "**Declare imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 41565,
     "status": "ok",
     "timestamp": 1765096207307,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "SQBOn_lV6zjc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV09Xox5aYmA"
   },
   "source": [
    "**set model_id and BitsAndBytes config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765096901155,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "w5K7O06sbny_"
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSl_j0GRaYh5"
   },
   "source": [
    "**Set LoRA config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1765096903834,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "wHqCQ-8wboX3"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44fEIWNCaYeR"
   },
   "source": [
    "**Tokenizer & load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "referenced_widgets": [
      "83d7bbf85b9d4e2cac0d94abfbed88c6",
      "28fa3191412d4ff0b2bfcfae6ca31336",
      "20ca3a53169a46d18463e4d5e4278e40",
      "f172d089c1254398985725239d63fc63",
      "a3c3fd40eb0f4bc88cda36e8e77cc8ea",
      "b17b8f2d961e4e86a458bbc23fb4f11f",
      "93bf8101257745baba7ca79719490929",
      "41c4ca2c0ec64019bfa72d20618348ab",
      "9367464f5d95433b91e6fba43ead998c",
      "78e05c8769b9491a89553e842fcdb57d",
      "149c0a14829847c18351402de3025088",
      "eeeffd917b014e418b49ef97456a2fde",
      "e1d66be04fca44f2a0d5acd0d373fe31",
      "e5a1c19a9872447db205e820136d104a",
      "fe3dfed78111446da954682d3c33141e",
      "46f6e0180b844f0ebeb46a71620b4972",
      "9340a27993734799be7f7dbc0f1be3d2",
      "187ce028d0d3457782a166971f5cc37b",
      "d148bff2a12d4e2eb6be994e5d6292e7",
      "93e11a4b8dc349e3b50f68cca9615e72",
      "999649299db6464e8b374d1c6d563be2",
      "3fdfc7e85f314d30b6e2e941410bf76d",
      "0e8d1fd4f3a642cb9408e2c69b5ff3d4",
      "e0e2cf3edc73448aa2fa59b8744de006",
      "6db7e11c856c4247837fa01d5ed1a2cb",
      "49d75c5138a649219275fb337f836428",
      "fe76cdcfb8144589968df84f34e55abf",
      "da181c7a51d14409bde44e92c556cd5e",
      "ea637d731c2543c9a2cc3bb9477adcfc",
      "a47acca11d7d49ff8d20279cea81cc45",
      "ea59a306976e4a15876680f70d48bbf3",
      "1deb9b0f17564c34855ef8e143655951",
      "1f8fa8030ac04026b204052d162ef503",
      "d3e87f1aaf1d40cd8d341292eeb4f38a",
      "163591248d884183a5d4257fb3c4a33f",
      "7821e19945bb455a96e242fd578344e2",
      "aa4309c39a324ff3ab36106bfaf93bff",
      "d3161154bffd4e2ba3f2c1449484ec79",
      "65acfabcb5554ddf86ecb25ae0a285c8",
      "3d0ccbefb4d04a7ca769f7f81f074b38",
      "6c2efbaccaf7470aba02c22fe5e25f88",
      "cf2afb82a8e0402d805a979f93f5b67d",
      "987b585ba96a4991b8770b9bcab352e9",
      "da12f12aa82b4e2f93925c79f3aed226",
      "7b6249e751f8448eb7318caf52ce841e",
      "dd04041ce85247b9a973d8705ee7ed40",
      "e7d00c1a1841453fb7836d7e0b43c0de",
      "caf0d94a05af40cfa354d1c89c2d881f",
      "d757b7211368425cb11f633dd3d37418",
      "c3618a52f8fa4eb0a02b674cde7898ad",
      "c7e9abba6c624ef2a36cdab187407c4f",
      "850a95dade534060b18b11e831f045d5",
      "82b35a6c54f247019491c48dcac1a82b",
      "ffcf0b96575643b6b5914423ffb40d05",
      "ee7f80808cc34c3e88b65dff61f4833e",
      "38c4c1927e3e46e68735916fdf0e9fa8",
      "573dd49a83824aa2a3c344154ed1caa6",
      "3e73bd0405b442388de977e6a19021df",
      "9d86b07fdd51477183b29fde2388d281",
      "79179420d72c4c558717648c01216472",
      "62f5fbb4a3d545c9bdf4a0ce7c3b9eeb",
      "9520dd579f2e442f9ebfa1ed99fb9ebf",
      "8ecfe1bb06ff4adc83311fb2254d0312",
      "470b48ecceb54e879ed9d8a1df24a2ea",
      "a33db4b5b925478fb76b9f0b97240b06",
      "362a79b4211d4794a707b4a8ceb14a6f"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 36521,
     "status": "ok",
     "timestamp": 1765096945964,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "gzWLEcfdbo-4",
    "outputId": "18f33332-5206-409a-ca4d-6ad63ce4a473"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d7bbf85b9d4e2cac0d94abfbed88c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeeffd917b014e418b49ef97456a2fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8d1fd4f3a642cb9408e2c69b5ff3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e87f1aaf1d40cd8d341292eeb4f38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6249e751f8448eb7318caf52ce841e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c4c1927e3e46e68735916fdf0e9fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map = {'':0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo3ajERDaYaZ"
   },
   "source": [
    "**load required dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278,
     "referenced_widgets": [
      "e6e6b13532494c88b4957d6ef87ae75c",
      "9195f27ff8e8406c9d68b374b1403ce3",
      "1d5bfe393521475a927fcaec00333944",
      "a171ac61db2242c4bb5be68428e8517f",
      "1d406f197e074233a04e3a94bcf5b5c6",
      "506dd6d6987f44f69fcc2c9f350cca78",
      "fd5eca32c34942aead00fd2af95085a7",
      "b80dc0cbc2aa4957b9d61eab4a60fdd1",
      "04e92e9f5fc94e19887012a0aeae5b0f",
      "5f3d790998a846dabd7c798852e381a3",
      "6b8043670a114bc1a78fdc17f900539f"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1415,
     "status": "error",
     "timestamp": 1765098261900,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "Ad7dInQ6bpb3",
    "outputId": "6e69eaec-9566-4044-9c47-bcb16b88b425"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"HiuXB/Welding-Handbook-QA\")\n",
    "dataset = data.map(lambda samples: tokenizer(['quetion','answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEvAy5XrbM3X"
   },
   "source": [
    "**prepare data for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765099973900,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "7kGZeACRbp3P"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "  text = f\"Quetion: {example['question']}\\nAnswer :{example['answer']}{tokenizer.eos_token}\"\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1765099975544,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "A0zWXmWaBM07",
    "outputId": "c5d00bca-db83-4803-e1a9-05708c897658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'id'],\n",
       "    num_rows: 16389\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9gRxmCNbMtA"
   },
   "source": [
    "**set training arg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239,
     "referenced_widgets": [
      "dfac79f41c01497ba667aae4c55fbffa",
      "02f88193f92947a89db32e2d76093caf",
      "66dc8bd59aab4a2792021c5de981668b",
      "62ca56ef086b4c4baff423ad98611734",
      "257cafef7ef941fc97109b8af1b71ca1",
      "69e98925ac8a47baa0a3be32853918fb",
      "bc14410691ad495a86005ec48a3d7e6d",
      "96ec3c81decc432280706fe15f90655b",
      "c74d94038b4e436b989e872b98ac6013",
      "27c49928131842beb81870b08702542b",
      "3037d6c17a78451eaebf6686020caca6",
      "5adf7ace84b5429bbfe6a04cc8a9f828",
      "d86c67e4c51e4b68864d592503cafeee",
      "169dced8f9904ed7870ee7267bd70a35",
      "1bf16d1673d34c1dad7bdeb9874e50dd",
      "f23d245725e3411cb31f0f23db059361",
      "5892104dae1047d3b685e774a30c5da4",
      "c4c7cba3ae6f400b9a0201f245da70ff",
      "540f14c05bff4aa39307de2150cdc206",
      "9009905e54534454b3f1ff9a5d5ed8ce",
      "69bcdebfa7b44ee699d81b3c07c457de",
      "a49c223e5aa241828857db2f27d7bb2b",
      "44c3d0c301d7471cbe8b9496cc9e6708",
      "2080da8c11cc4c4fa85b9c5b8a277de2",
      "2f2d24d12417483cad16b8a6d0e9b9f2",
      "ebd076ffca484bf5920e1e4058f76956",
      "1257233c353a4121b85e6fc060b12631",
      "71a6a8c1f4ba48fb8a1e97c0f46e6313",
      "3b4086a09b69415ba5b7c34c3035a00b",
      "6c55e43e5231453b94f5755cb38c3419",
      "8951dc232edd4d94b76b935f73ecabd5",
      "f81ef0f0395b40509af219c2dd322857",
      "7a431552e5dd4b04b6f51abb7cddb500",
      "83c76175546546599d4866a03128a451",
      "7de044e9e6cc4a06b670ea0ee11e3603",
      "2b3874b67f04415896de25a0fe36a143",
      "973ab38d6528444fbf85af2656b98a7d",
      "d4c1f8774a674268a850ed00e224b072",
      "97d0e8a6d0ad4ab2bbb9843d352cc4c8",
      "31951e9f223e412c9ee9e46b157b5f92",
      "2a73adc13ddb41dc9f2a5d0c615be8b1",
      "7f783f46d4c546c2b63420050a04ea36",
      "9c23464f87c24171a9b91953c59278bb",
      "c399dc146bd64e1c99c105166f6a551e"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66217,
     "status": "ok",
     "timestamp": 1765100044903,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "B4sgj1SnbqeQ",
    "outputId": "d98ccbf1-52cf-4363-b5ae-e47044fd8c42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfac79f41c01497ba667aae4c55fbffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/16389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adf7ace84b5429bbfe6a04cc8a9f828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/16389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3d0c301d7471cbe8b9496cc9e6708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c76175546546599d4866a03128a451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/16389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset = data['train'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        warmup_steps=2,\n",
    "        max_steps=100,\n",
    "        fp16=True,\n",
    "        optim='paged_adamw_8bit',\n",
    "        output_dir='output',\n",
    "        report_to='none'\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqvQDuHrbXvv"
   },
   "source": [
    "**initialize training train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 171453,
     "status": "ok",
     "timestamp": 1765100293097,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "NQylbz8HIq_W",
    "outputId": "48f886e1-2af6-4f28-90a5-ba6d72073090"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.670100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.904300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.878500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.161100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.790100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.135900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.0310260784626006, metrics={'train_runtime': 170.5645, 'train_samples_per_second': 2.345, 'train_steps_per_second': 0.586, 'total_flos': 721557257625600.0, 'train_loss': 2.0310260784626006, 'epoch': 0.024406614192446153})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfjbDem70woV"
   },
   "source": [
    "**Merge trained model into base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 31313,
     "status": "ok",
     "timestamp": 1765101141827,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "3_2SSJqa05-u",
    "outputId": "b1db8d0d-8639-4779-bd5a-2264e702d132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged model saved at: ./output/FinalModel\n"
     ]
    }
   ],
   "source": [
    "final_model=PeftModel.from_pretrained(model,\n",
    "                                      \"./output/checkpoint-100\")\n",
    "# Correct method to merge LoRA weights into the base model\n",
    "merge_model = final_model.merge_and_unload()\n",
    "\n",
    "save_path = \"./output/FinalModel\"\n",
    "merge_model.save_pretrained(save_path)\n",
    "AutoTokenizer.from_pretrained(model_id).save_pretrained(save_path)\n",
    "\n",
    "print(f\"✅ Merged model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGnE0rCGbhpv"
   },
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16270,
     "status": "ok",
     "timestamp": 1765102462592,
     "user": {
      "displayName": "Raj Tembe",
      "userId": "12019527906788798153"
     },
     "user_tz": -330
    },
    "id": "bPG4bNYGbreY",
    "outputId": "407b2a5d-a7c4-49bb-d9e2-514960e5de2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './output/FinalModel' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the transient liquid-phase (TLP) bonding method used for joining silicon-base ceramics and describe why it is more challenging to adapt TLP bonding when joining ceramics with a metallic interlayer. Include in your answer the role of the tri-layer interlayer design and the key factors that influence the isothermal solidification of the liquid phase in these joints.\n",
      "Answer: Transient liquid-phase (TLP) bonding method used for joining silicon-base ceramics and describe why it is more challenging to adapt TLP bonding when joining ceramics with a metallic interlayer. Include in your answer the role of the tri-layer interlayer design and the key factors that influence the isothermal solidification of the liquid phase in these joints.\n",
      "Transitive liquid-phase (TLP) bonding method used for joining silicon-base ceramics and describe why it is more challenging to adapt TLP bonding when joining ceramics with a metallic interlayer. Include in your answer the role of the tri-layer interlayer design and the key factors that influence the isothermal\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"./output/FinalModel\")\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./output/FinalModel\",\n",
    "    device_map={'':0}\n",
    ")\n",
    "\n",
    "prompt = \"\"\"Explain the transient liquid-phase (TLP) bonding method used for joining silicon-base ceramics and describe why it is more challenging to adapt TLP bonding when joining ceramics with a metallic interlayer. Include in your answer the role of the tri-layer interlayer design and the key factors that influence the isothermal solidification of the liquid phase in these joints.\\nAnswer:\"\"\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "\n",
    "out = mdl.generate(**inputs, max_new_tokens=128)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPS8cVEzbQbLM1mymoE/3M2",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
